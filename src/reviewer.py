import os
import re
import json
import textwrap
import yaml
import time
import fnmatch
import logging
from typing import List, Dict, Any, Optional, Tuple
from openai import OpenAI
from github import Github, Auth
import argparse

DEFAULT_MAX_DIFF_CHARS = 8000
DEFAULT_MAX_FINDINGS = 50
DEFAULT_BATCH_SIZE = 20
DEFAULT_MODEL = "gpt-5"

SEVERITY_EMOJI = {
    "CRITICAL": "ğŸ”´",
    "MAJOR": "ğŸŸ ",
    "MINOR": "ğŸŸ¡",
    "SUGGESTION": "ğŸŸ¢",
}
SEVERITY_ORDER = ["SUGGESTION", "MINOR", "MAJOR", "CRITICAL"]


def _get(obj: Any, key: str, default: Any = None) -> Any:
    if isinstance(obj, dict):
        return obj.get(key, default)
    return getattr(obj, key, default)


def extract_output_text(resp: Any) -> str:
    """
    OpenAI Responses API ã®è¿”å´ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚
    output_text ãŒç„¡ã„å ´åˆã‚‚æƒ³å®šã—ã€content ã® text ã‚’èµ°æŸ»ã™ã‚‹ã€‚
    """
    text = _get(resp, "output_text")
    if text:
        return str(text)
    output = _get(resp, "output")
    parts: List[str] = []
    if output:
        for item in output:
            content = _get(item, "content")
            if not content:
                continue
            for block in content:
                piece = _get(block, "text")
                if piece:
                    parts.append(str(piece))
    if parts:
        return "\n".join(parts)
    return ""


def load_config() -> dict:
    base_dir = os.path.dirname(os.path.abspath(__file__))
    cfg_path = os.path.join(base_dir, "config.yaml")
    with open(cfg_path, "r", encoding="utf-8") as f:
        raw = f.read()
    expanded = os.path.expandvars(raw)
    return yaml.safe_load(expanded)


def retry(fn, tries: int = 3, base_sleep: float = 1.0):
    """æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ä»˜ããƒªãƒˆãƒ©ã‚¤"""
    for i in range(tries):
        try:
            return fn()
        except Exception as e:
            if i == tries - 1:
                raise
            time.sleep(base_sleep * (2 ** i))


def extract_json_block(text: str) -> Optional[str]:
    """```json ... ``` ã‚’æŠœãå‡ºã™"""
    m = re.search(r"```json\s*(.+?)\s*```", text, re.DOTALL | re.IGNORECASE)
    return m.group(1) if m else None


def to_inline_body(f: Dict[str, Any]) -> str:
    """å„æŒ‡æ‘˜ã®æœ¬æ–‡ã‚’æ•´å½¢"""
    parts = [
        f'{SEVERITY_EMOJI[f["severity"]]} **{f["severity"]}** â€” {f["title"]}',
        (f.get("detail") or ""),
        (f'**ä¿®æ­£æ¡ˆ:** {f["fix"]}' if f.get("fix") else "")
    ]
    return "\n\n".join([p for p in parts if p]).strip()


def build_prompt(files, user_prompt: str, max_diff_chars: int, style: Optional[str] = None) -> str:
    filenames = [f.filename for f in files]
    file_list = "\n".join(f"- {name}" for name in filenames)

    patches, used = [], 0
    for f in files:
        patch = f.patch or ""
        block = f"\n\n=== {f.filename} ===\n{patch}"
        block_len = len(block)
        if used + block_len > max_diff_chars:
            remaining = max_diff_chars - used
            if remaining > 0:
                patches.append(block[:remaining])
                used += remaining
            break
        patches.append(block)
        used += block_len
    diff_snippet = "".join(patches) if patches else "(å¤‰æ›´å·®åˆ†ã¯å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ)"
    style_directive = f"\nãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯ã€Œ{style}ã€ãªãƒˆãƒ¼ãƒ³ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚" if style else ""

    return textwrap.dedent(f"""
    ã‚ãªãŸã¯ç†Ÿç·´ã—ãŸã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ã—ã¦ã€ä»¥ä¸‹ã®PRå·®åˆ†ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ãã ã•ã„ã€‚{style_directive}
    å‡ºåŠ›ã¯å¿…ãš ```json ãƒ•ã‚§ãƒ³ã‚¹å†…ã« JSONé…åˆ—ã®ã¿``` ã§è¿”ã—ã¦ãã ã•ã„ã€‚

    JSONã‚¹ã‚­ãƒ¼ãƒ:
    [
      {{
        "severity": "CRITICAL" | "MAJOR" | "MINOR" | "SUGGESTION",
        "file": "ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆä¾‹: src/main.pyï¼‰",
        "line": 123,  // å³å´(HEAD)ã®è¡Œç•ªå·ã‚’è¿”ã™ã“ã¨
        "title": "çŸ­ã„è¦‹å‡ºã—",
        "detail": "èƒŒæ™¯/æ ¹æ‹ ã‚’ç°¡æ½”ã«è¨˜è¼‰",
        "fix": "å…·ä½“çš„ãªä¿®æ­£æ¡ˆï¼ˆä»»æ„ï¼‰"
      }}
    ]

    ã€å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ã€‘
    {file_list}

    ã€å·®åˆ†ï¼ˆä¸Šé™ {max_diff_chars} æ–‡å­—ï¼‰ã€‘
    {diff_snippet}

    ã€è¿½åŠ æŒ‡ç¤ºã€‘
    {user_prompt or '(ç‰¹ã«ãªã—)'}
    """).strip()


def normalize_findings(data: Any, max_findings: int) -> List[Dict[str, Any]]:
    findings: List[Dict[str, Any]] = []
    if isinstance(data, dict):
        data = [data]
    if not isinstance(data, list):
        return findings

    for item in data[:max_findings]:
        if not isinstance(item, dict):
            continue
        sev = str(item.get("severity", "SUGGESTION")).upper().strip()
        if sev not in SEVERITY_EMOJI:
            sev = "SUGGESTION"
        try:
            line = int(item.get("line")) if item.get("line") else None
        except Exception:
            line = None
        findings.append({
            "severity": sev,
            "file": item.get("file", "-"),
            "line": line,
            "title": (item.get("title") or "").strip() or "ï¼ˆã‚¿ã‚¤ãƒˆãƒ«æœªè¨­å®šï¼‰",
            "detail": (item.get("detail") or "").strip(),
            "fix": (item.get("fix") or "").strip(),
        })
    return findings


def filter_files(files, include_globs, exclude_globs, max_files):
    result = []
    for f in files:
        path = f.filename
        if include_globs and not any(fnmatch.fnmatch(path, p) for p in include_globs):
            continue
        if exclude_globs and any(fnmatch.fnmatch(path, p) for p in exclude_globs):
            continue
        if f.patch is None:
            continue
        result.append(f)
        if len(result) >= max_files:
            break
    return result


def build_position_map(files) -> Dict[str, Dict[int, int]]:
    """
    å„ãƒ•ã‚¡ã‚¤ãƒ«ã® unified diff ã‚’è§£æã—ã€
    å³å´(æ–°ãƒ•ã‚¡ã‚¤ãƒ«)ã®è¡Œç•ªå· -> diffå†…position ã®ãƒãƒƒãƒ—ã‚’ä½œã‚‹ã€‚
    position ã¯ GitHub API ã® review comment ã§ä½¿ã†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚
    """
    maps: Dict[str, Dict[int, int]] = {}
    for f in files:
        patch = f.patch
        if not patch:
            continue
        right_line = 0
        # hunk å…ˆé ­ã® "+<start>,<len>" ã‚’è§£æ
        position = 0  # diffå†…ã®ä½ç½®ã¯1å§‹ã¾ã‚Šã§ã‚«ã‚¦ãƒ³ãƒˆ
        mapping: Dict[int, int] = {}

        for raw in patch.splitlines():
            position += 1
            if raw.startswith('@@'):
                # ä¾‹: @@ -12,7 +20,6 @@
                m = re.search(r"\+(\d+)(?:,(\d+))?", raw)
                if m:
                    right_line = int(m.group(1))
                else:
                    right_line = 0
                # ãƒ˜ãƒƒãƒ€è¡Œè‡ªä½“ã‚‚positionã«å«ã¾ã‚Œã‚‹ï¼ˆä¸Šã§+1æ¸ˆã¿ï¼‰
                continue
            if raw.startswith('+'):  # è¿½åŠ è¡Œï¼ˆå³å´ã®ã¿é€²ã‚€ï¼‰
                mapping[right_line] = position
                right_line += 1
            elif raw.startswith('-'):  # å‰Šé™¤è¡Œï¼ˆå·¦å´ã®ã¿é€²ã‚€ï¼‰
                # å³å´ã®è¡Œç•ªå·ã¯é€²ã‚ãªã„
                pass
            else:
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡Œï¼šä¸¡å´é€²ã‚€
                if right_line > 0:
                    right_line += 1

        if mapping:
            maps[f.filename] = mapping
    return maps


def find_position(pos_map: Dict[str, Dict[int, int]], path: str, line: Optional[int], snap_range: int = 3) -> Optional[int]:
    """
    æŒ‡å®šã® path/line(å³å´)ã«æœ€ã‚‚è¿‘ã„ position ã‚’æ¢ã™ã€‚
    ãã®è¡ŒãŒè¿½åŠ è¡Œã§ãªã„å ´åˆã‚‚ã‚ã‚‹ã®ã§ã€è¿‘å‚ã®è¿½åŠ è¡Œã«ã‚¹ãƒŠãƒƒãƒ—ã™ã‚‹ã€‚
    """
    if line is None:
        return None
    m = pos_map.get(path)
    if not m:
        return None
    if line in m:
        return m[line]
    # è¿‘å‚æ¢ç´¢
    for d in range(1, snap_range + 1):
        if (line - d) in m:
            return m[line - d]
        if (line + d) in m:
            return m[line + d]
    return None


def dedup_existing(pr, inline_candidates, fallback_texts):
    """æ—¢å­˜ã‚³ãƒ¡ãƒ³ãƒˆé‡è¤‡é˜²æ­¢ï¼ˆpositionåŸºæº–ã‚’å„ªå…ˆï¼‰"""
    existing_inline = {}
    for c in pr.get_review_comments():
        key = (c.path, c.position or c.line, (c.body or "").strip())
        existing_inline[key] = True

    existing_reviews = {(r.body or "").strip() for r in pr.get_reviews() if (r.body or "").strip()}

    filtered_inline = []
    for c in inline_candidates:
        key = (c["path"], c.get("position") or c.get("line"), c["body"].strip())
        if key in existing_inline:
            continue
        filtered_inline.append(c)

    filtered_fallback = [b for b in fallback_texts if b.strip() not in existing_reviews]
    return filtered_inline, filtered_fallback


def post_inline_reviews(pr, findings, batch_size):
    # å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ï¼ˆpositionãƒãƒƒãƒ—ä½œæˆç”¨ï¼‰
    changed_files = list(pr.get_files())
    changed_paths = {f.filename for f in changed_files}
    pos_map = build_position_map(changed_files)

    inline, fallback_lines = [], []

    for f in findings:
        path, line = f["file"], f["line"]
        if path in changed_paths:
            pos = find_position(pos_map, path, line)
        else:
            pos = None

        body = to_inline_body(f)

        if pos is not None:
            # âœ… position ã‚’ä½¿ã£ã¦ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆ422å›é¿ï¼‰
            inline.append({"path": path, "position": pos, "body": body})
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã¾ã¨ã‚ã‚³ãƒ¡ãƒ³ãƒˆã«å›ã™
            where = f'`{path}`' + (f' L{line}' if line else "")
            fallback_lines.append(
                f'- {SEVERITY_EMOJI[f["severity"]]} **{f["severity"]}** {where} â€” {f["title"]}\n'
                f'  {f["detail"]}' + (f'\n  **ä¿®æ­£æ¡ˆ:** {f["fix"]}' if f["fix"] else "")
            )

    fallback_body = None
    if fallback_lines:
        body_content = "\n".join(fallback_lines)
        fallback_body = "### ğŸ¤– AIãƒ¬ãƒ“ãƒ¥ãƒ¼Botï¼ˆè¡Œç‰¹å®šä¸å¯ã®æŒ‡æ‘˜ï¼‰\n\n" + (body_content or "å†…å®¹ãªã—")

    inline, fallback_bodies = dedup_existing(pr, inline, [fallback_body] if fallback_body else [])

    # ãƒãƒƒãƒã§ãƒ¬ãƒ“ãƒ¥ãƒ¼ä½œæˆ
    for i in range(0, len(inline), batch_size):
        batch = inline[i:i + batch_size]
        if not batch:
            continue
        retry(lambda: pr.create_review(body="", event="COMMENT", comments=batch))

    if fallback_bodies:
        retry(lambda: pr.create_review(body=fallback_bodies[0], event="COMMENT"))


def maybe_fail_job(findings, fail_level):
    if not fail_level:
        return
    level = fail_level.upper().strip()
    if level not in SEVERITY_ORDER:
        return
    worst = "SUGGESTION"
    for f in findings:
        if SEVERITY_ORDER.index(f["severity"]) > SEVERITY_ORDER.index(worst):
            worst = f["severity"]
    if SEVERITY_ORDER.index(worst) >= SEVERITY_ORDER.index(level):
        raise SystemExit(1)


def build_no_findings_body(raw_text: str, parsed_successfully: bool) -> str:
    header = "### ğŸ¤– AIãƒ¬ãƒ“ãƒ¥ãƒ¼Bot"
    if parsed_successfully:
        return f"{header}\n\nLGTM! ğŸ‰ ç‰¹ã«æŒ‡æ‘˜ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
    message = (raw_text or "").strip()
    if not message:
        message = "ãƒ¬ãƒ“ãƒ¥ãƒ¼å†…å®¹ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ï¼ˆãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªå¿œç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼‰"
    return f"{header}\n\n{message}"


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--repo", required=True)
    parser.add_argument("--pr", required=True)
    parser.add_argument("--prompt", default="")
    args = parser.parse_args()

    cfg = load_config()
    log_level_name = str(cfg.get("log_level") or "INFO").upper()
    log_level = getattr(logging, log_level_name, logging.INFO)
    logging.basicConfig(level=log_level, format="%(asctime)s [%(levelname)s] %(message)s")
    logging.info("AIãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’é–‹å§‹ã—ã¾ã™: repo=%s, pr=%s", args.repo, args.pr)
    openai_key = cfg.get("openai_api_key") or os.getenv("OPENAI_API_KEY")
    gh_token = cfg.get("github_token") or os.getenv("GITHUB_TOKEN")
    if not openai_key:
        raise RuntimeError("OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    if not gh_token:
        raise RuntimeError("GITHUB_TOKEN ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    model = cfg.get("model", DEFAULT_MODEL)
    system_prompt = (cfg.get("system_prompt") or "").strip()
    style = (cfg.get("style") or "").strip()
    enable_inline = bool(cfg.get("enable_inline", True))
    fail_level = cfg.get("fail_level")
    include_globs = cfg.get("include_globs", []) or []
    exclude_globs = cfg.get("exclude_globs", []) or []
    max_files = int(cfg.get("max_files", 200))
    max_diff_chars = int(cfg.get("max_diff_chars", DEFAULT_MAX_DIFF_CHARS))
    max_findings = int(cfg.get("max_findings", DEFAULT_MAX_FINDINGS))
    batch_size = int(cfg.get("batch_size", DEFAULT_BATCH_SIZE))
    max_output_tokens = cfg.get("max_tokens")
    if max_output_tokens:
        try:
            max_output_tokens = int(max_output_tokens)
        except (TypeError, ValueError):
            max_output_tokens = None

    gh = Github(auth=Auth.Token(gh_token))
    repo = gh.get_repo(args.repo)
    pr = repo.get_pull(int(args.pr))

    # ãƒ‰ãƒ©ãƒ•ãƒˆPRã¯ã‚¹ã‚­ãƒƒãƒ—
    if getattr(pr, "draft", False):
        logging.info("PR #%s ã¯ãƒ‰ãƒ©ãƒ•ãƒˆã®ãŸã‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚", args.pr)
        return

    files_all = list(pr.get_files())
    files = filter_files(files_all, include_globs, exclude_globs, max_files)
    if not files:
        logging.info("å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ•ç¨¿ã—ã¾ã™ã€‚")
        retry(lambda: pr.create_review(body="### ğŸ¤– AIãƒ¬ãƒ“ãƒ¥ãƒ¼Bot\n\nå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", event="COMMENT"))
        return

    logging.info("ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: %s (å–å¾— %s, ä¸Šé™ %s)", len(files), len(files_all), max_files)

    prompt_text = build_prompt(files, args.prompt, max_diff_chars, style=style or None)
    client = OpenAI(api_key=openai_key)
    messages: List[Dict[str, str]] = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt_text})
    request_kwargs: Dict[str, Any] = {"model": model, "input": messages}
    if max_output_tokens:
        request_kwargs["max_output_tokens"] = max_output_tokens
    raw_text = ""
    for attempt in range(1, 4):
        resp = retry(lambda: client.responses.create(**request_kwargs))
        raw_text = extract_output_text(resp)
        if raw_text.strip():
            break
        logging.warning("OpenAIãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã§ã—ãŸã€‚ï¼ˆè©¦è¡Œ %s/3ï¼‰", attempt)
    else:
        logging.error("OpenAIãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒ3å›é€£ç¶šã§ç©ºã§ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
        raise RuntimeError("OpenAI responses were empty after multiple attempts.")
    if logging.getLogger().isEnabledFor(logging.DEBUG):
        logging.debug("OpenAI raw response: %r", resp)

    findings = []
    parsed_successfully = False
    json_block = extract_json_block(raw_text)
    if json_block:
        try:
            data = json.loads(json_block)
            findings = normalize_findings(data, max_findings)
            parsed_successfully = True
            logging.info("JSON ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æã—ã¾ã—ãŸã€‚æŒ‡æ‘˜æ•°: %s", len(findings))
        except Exception as exc:
            logging.warning("JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", exc)
            findings = []
    else:
        snippet = (raw_text[:300] + "â€¦") if raw_text and len(raw_text) > 300 else (raw_text or "(ç©º)")
        logging.warning("ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‹ã‚‰ JSON ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å‡ºåŠ›(å…ˆé ­300æ–‡å­—): %s", snippet)

    if not findings:
        review_body = build_no_findings_body(raw_text, parsed_successfully)
        retry(lambda: pr.create_review(body=review_body, event="COMMENT"))
        logging.info("æŒ‡æ‘˜ãªã—ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ•ç¨¿ã—ã¾ã—ãŸã€‚ï¼ˆparsed=%sï¼‰", parsed_successfully)
        return

    if enable_inline:
        logging.info("ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ %s ä»¶ã®æŒ‡æ‘˜ã‚’æŠ•ç¨¿ã—ã¾ã™ã€‚", len(findings))
        post_inline_reviews(pr, findings, batch_size)
    else:
        logging.info("ã¾ã¨ã‚ã‚³ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ %s ä»¶ã®æŒ‡æ‘˜ã‚’æŠ•ç¨¿ã—ã¾ã™ã€‚", len(findings))
        bullets = []
        for f in findings:
            where = f'`{f["file"]}`' + (f' L{f["line"]}' if f["line"] else "")
            bullets.append(f'- {SEVERITY_EMOJI[f["severity"]]} **{f["severity"]}** {where} â€” {f["title"]}\n  {f["detail"]}')
            if f["fix"]:
                bullets.append(f'  **ä¿®æ­£æ¡ˆ:** {f["fix"]}')
        retry(lambda: pr.create_review(body="### ğŸ¤– AIãƒ¬ãƒ“ãƒ¥ãƒ¼Bot\n\n" + "\n".join(bullets), event="COMMENT"))

    maybe_fail_job(findings, fail_level)


if __name__ == "__main__":
    main()
